temporal locality:The locality principle stating that if a data location is referenced then it will tend to be referenced again soon.

spatial locality:The locality principle stating that if a data location is referenced,data locations with nearby addresses will tend to be referenced soon..

memory hierarchy:A structure that uses multiple levels of memories;as the distance from the processor increases,the size of the memories and the access time both increase.

![image](images/561706AE03F14999BC7599F92A1366F11600740906(1).png)

block (or line):The minumum unit of information that can be either present or not present in a cache.

![image](images/2831392F3C914F6BAD6EED041F9A03DC1600741399(1).png)

# Memory Technologies

![image](images/DB45F6BE46FA409AB54B64B1997A6A491600744180(1).png)

### SRAM Technology

Don't need to refresh

### DRAM Technology

need refresh

![image](images/01645ACFDC5942179B462CD42DA567D71600746046(1).png)

# The Basics of Caches

### Direct-mapped cache

A cache structure in which each memory location is mapped to exactly one location in the cache.

![image](images/5F5D3CBEC7154BE192EC3CD0245B190C1600756049(1).png)

![image](images/47C0E98F6C594568BB803C3EFBC09A771600756072(1).png)

Tag: A field in a table used for a memory hierarchy that contains the address information required to identify whether the associated block in the hierarchy corresponds to a requested word.The tag needs just to contain the upper portion of the address,corrsponding to the bits that are not used as an index into the cache.

valid bit:A field in the tables of a memory hierarchy that indicates that the associated block in the hierarchy contains valid data.

### Accessing a Cache

shows how a referenced address is divided into

- A tag field,which is used to compare with the value of the tag field of the cache.
- A cache index,which is used to select the block

![image](images/65632FBCD3A448CEB5C229CFB9BA28D91600826331(1).png)

![image](images/E9C310E512414A80865C5542F28760751600828726(1).png)

### Handing Cache Misses

For a cache miss,we can stall the entire processor,essentially freezing the content of the temporary and programmer-visible registers,while we wait for memory.

steps to taken on an instruction cache miss:

- Send the original PC value to the memory.
- Instruct main memory to perform a read and wait for the memory to complete its access.
- Write the cache entry,putting the data from memory in the data portion of the entry,writing the uppers bits of the address into the tag field,and turning the valid bit on.
- Restart the instruction execution at the first step,which will refetch the instruction,this time finding it in the cache.

### Handing Writes

write-through

write-buffer

write-back

What occurs on a write miss: We first fetch the words of the block from memory.After the block is fetched and placed into the cache,we can overwrite the word that caused the miss into the cache block.We also write the word to main memory using the full address.

# Measuring and Improving Cache Performance

![image](images/E072EDA89C2C459593ECA99C4A9496E81600842707(1).png)

![image](images/00953F10EF0E42D1A83746CB640F6CC11600842824(1).png)

### Reducing Cache Misses by More Flexible Placement of Blocks

fully associative cache:A cache structure in which a block can be placed in any location in the cache.

set associative:A set-associative cache with n locations for a block is called an n-way set-associative cache.An n-way set-associative cache consists of a number of sets,each of which consists of n blocks.Each block in the memory maps to a unique set in the cache given by the index field,and a block can be placed in any element of that set.Thus,a set-associative placement combines direct-mapped placement and fully associative placement:a block is directly mapped into a set,and then all the blocks in the set are searched for a match.

![image](images/0165638809AE4B05A935777C3B2FE71B1600912899(1).png)

A direct-mapped cache is just a one-way set-associative cache:each cache entry holds one block and each set has one element.A fully associative cache with m entries is simple an m-way set-associative cache;it has one set with m blocks,and an entry can reside in any block within that set.

### Loacting a Block in the Cache

Now,let's consider the task of finding a block in a cache this is set associative.

![image](images/1D86E272E8414AAEB2CC0B2A66AAB43B1600915522(1).png)

![image](images/E285D094D2374741B8BC31878EA09BF31600916027(1).png)

### Choosing Which Block to Replace

LRU

### Reducing the Miss Penalty Using Multilevel Caches

multilevel cache

First-level caches are more concerned about hit time,and second-level caches are more concerned about miss rate.

### Software Optimization via Blocking

### Summary

To reduce the miss rate,we examined the use of associative placement schemes.

We looked at multilevel caches as a technique to reduce the miss penalty by allowing a larger secondary cache to handle misses to the primary cache.

# Virtual Machines

- Reduce the cost of processor virtualization.
- Reduce interrupt overhead cost due to the virtualization.
- Reduce interrupt cost by steering(转移) interrupts to the proper VM without invoking VMM.

# Virtual Memory

### TLB(translation lookaside buffer)

Since the page tables are stored in main memory,every memory access by a program can take at least twice as long:one memory access to obtain the physical address and a second access to get the data.The key to improving access performance is to rely on locality of reference to the page table.When a translation for a virtual page number is used,it will probably be needed again soon,because the references to the words on that page have both temporal and spatial locality.

The TLB simply loads the physical address and protection tags from the last level page table.

![image](images/FCD101870B2A480F88AED17DD7C7F8D21601015884(1).png)

After a TLB miss occurs and the missing translation has been retrieved from the page table,we will need to select a TLB entry to replace.Because the reference and dirty bits are contained in the TLB entry,we need to copy these bit back to page table entry when we replace an entry.These bits are the only portion of the TLB entry that can be changed.Using write-back -- this,copying these entries back at miss time rather than when they are written - is very efficient,since we expect the TLB miss rate to be small


![image](images/FA588EFF68F049408E5B900E2F1E40AE1601171997(1).png)

### Integrating Virtual Memory,TLBs,and Caches

Our virtual memory and cache systems work together as a hierarchy,so that data cannot be in the cache unless it is present in main memory.

Under the best of circumstances(环境),a virtual address is translated by the TLB and sent to the cache where the appropriate data are found,retrieved,and sent back to the processor.In the worst case,a reference can miss in all three components of the memory hierarchy:The TLB,the page table,and the cache.

![image](images/5D57A883651F43038EAC8B7520BAD6E4clipboard.png)

### Implementing Protection with Virtual Memory

address space ID(ASID):reduce TLB flush when process context switch

### Handing TLB Misses and Page Faults

TLB miss can indicate one of two possibilities:

- The page is present in memory,and we need only create the missing TLB entry.
- The page is not present in memory,and we need to transfer control to operating system to deal with a page fault.

Page fault exceptions for data accesses are difficult to implement properly in a processor because of a combination of three:

- They occur in the middle of instructions,unlike instruction page fault.
- The instruction cannot be completed before handling the exception
- After handling the exception,the instruction must be restarted as if nothing had occurred.

# A common Framework for Memory Hierarchy

![image](images/3E9D97FA57B348CF8536BA8DB7715BEBclipboard.png)

![image](images/4A7768A718DF410EA2BF7D4F87EA635Fclipboard.png)

### The Three Cs:An Intuitive(直观) Model for Understanding the Behavior of Memory Hierarchies

We look at a model that provides insight into the sources of misses in a memory hierarchy and how the misses will be affected by changes in the hierarchy.

In this model,all misses are classified into one of three categories:

- Compulsory misses:These are cache misses caused by the first access to a block that has never been in the cache.  
- Capacity misses:These are cache misses caused when the cache cannot contain all the blocks needed during execution of a program.Capacity misses occur when blocks are replaced and later retrieved.
- Conflict misses:These are cache misses that occur in set-associative or direct-mapped caches when multiple blocks compete for the same set.

![image](images/57A2CF44879C4C3A84C7AE93B60C4FB31601264507(1).png)

![image](images/CB2697F8D9AB4DF68D5C66BEF1EF7CD9clipboard.png)

# Using a Finite-State Machine to Control a Simple Cache

We can now build control for a cache.

![image](images/3BDEA734DE094348822B138D4B7653EBclipboard.png)

### FSM for a Simple Cache Controller

![image](images/DA694ADF5C2741B49BAF7C760EE0ACD8clipboard.png)

# Parallelism and Memory Hierarchy: Cache Coherence

Cache一致性协议之MESI

https://blog.csdn.net/muxiqingyang/article/details/6615199